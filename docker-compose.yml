services:
  whisper:
    image: jjajjara/rocm-whisper-api:latest # Ensure this matches your pushed Docker Hub image tag (e.g., your_username/rocm-whisper-api:latest)
    container_name: rocm-whisper-api
    restart: unless-stopped
    ports:
      - "10300:3000" # Maps container port 8080 on the host to container port 8080
    environment:
      - HSA_OVERRIDE_GFX_VERSION=11.5.1 # <<< IMPORTANT: Adjust this based on your specific AMD GPU model if needed
      - WHISPER_MODEL=medium             # Change to 'small', 'medium', 'large', etc., as required
    devices:
      - "/dev/kfd:/dev/kfd" # Essential for ROCm Kernel Fusion Device access
      - "/dev/dri:/dev/dri" # Essential for Direct Rendering Infrastructure access for GPU
    # (Optional) Volume setup for model caching.
    # Uncomment the following lines to persist downloaded Whisper models on your host PC
    # to prevent re-downloading on container restarts and save bandwidth/time.
    # volumes:
    #   - ~/.cache/whisper:/root/.cache/whisper

  # wyoming-piper: (Commented out - migrated to Qwen3-TTS)
  # wyoming-piper:
  #   build:
  #     context: ./piper
  #     dockerfile: Dockerfile
  #     args:
  #       HSA_OVERRIDE_GFX_VERSION: ${HSA_OVERRIDE_GFX_VERSION:-11.5.1}
  #       # Build for multiple GPU architectures (RDNA 2 and RDNA 3)
  #       AMDGPU_TARGETS: gfx1151
  #   container_name: wyoming-piper-rocm
  #   restart: unless-stopped
  #
  #   # GPU device access - required for ROCm
  #   devices:
  #     - /dev/kfd:/dev/kfd
  #     - /dev/dri:/dev/dri
  #
  #   # Security options for GPU access
  #   security_opt:
  #     - seccomp:unconfined
  #
  #   # Group access for GPU
  #   group_add:
  #     - video
  #     - render
  #
  #   # Environment variables
  #   environment:
  #     - HSA_OVERRIDE_GFX_VERSION=${HSA_OVERRIDE_GFX_VERSION:-11.5.1}
  #     - HIP_VISIBLE_DEVICES=0
  #     - ROCM_VERSION=7.1.1
  #     - ROCM_PATH=/opt/rocm
  #     - LD_LIBRARY_PATH=/usr/local/lib:/opt/rocm/lib:/opt/rocm/lib/llvm/lib
  #     # ONNX Runtime threading - configured dynamically by piper_wrapper.py
  #     # Piper voice configuration
  #     - PIPER_VOICE=${PIPER_VOICE:-en_US-lessac-medium}
  #     - PIPER_LENGTH_SCALE=${PIPER_LENGTH_SCALE:-1.0}
  #     - PIPER_NOISE_SCALE=${PIPER_NOISE_SCALE:-0.667}
  #     - PIPER_NOISE_W=${PIPER_NOISE_W:-0.8}
  #     - PIPER_DEBUG=${PIPER_DEBUG:-false}
  #
  #   # Port for Wyoming protocol
  #   ports:
  #     - "10200:10200"
  #
  #   # Volume for model persistence
  #   volumes:
  #     - ./piper-data:/data
  #
  #   # Health check
  #   healthcheck:
  #     test: ["CMD", "python3", "-c", "import socket; s=socket.socket(); s.connect(('localhost',10200)); s.close()"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 3
  #     start_period: 60s

  # wyoming-qwen-tts: (Trying Kokoro first)
  # wyoming-qwen-tts:
  #   build:
  #     context: ./qwen3-tts
  #     dockerfile: Dockerfile
  #     args:
  #       HSA_OVERRIDE_GFX_VERSION: ${HSA_OVERRIDE_GFX_VERSION:-11.5.1}
  #   container_name: wyoming-qwen-tts-rocm
  #   restart: unless-stopped

  #   # GPU device access - required for ROCm
  #   devices:
  #     - /dev/kfd:/dev/kfd
  #     - /dev/dri:/dev/dri

  #   # Security options for GPU access
  #   security_opt:
  #     - seccomp:unconfined

  #   # Group access for GPU
  #   group_add:
  #     - video
  #     - render

  #   # Environment variables
  #   environment:
  #     - HSA_OVERRIDE_GFX_VERSION=${HSA_OVERRIDE_GFX_VERSION:-11.5.1}
  #     - HIP_VISIBLE_DEVICES=0
  #     - ROCM_VERSION=7.1.1
  #     - ROCM_PATH=/opt/rocm
  #     - LD_LIBRARY_PATH=/usr/local/lib:/opt/rocm/lib:/opt/rocm/lib/llvm/lib
  #     # Enable experimental ROCm SDPA optimizations for better performance
  #     - TORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL=1
  #     # MIOpen performance tuning - fix workspace allocation warnings
  #     - MIOPEN_USER_DB_PATH=/tmp/miopen-cache
  #     - MIOPEN_CUSTOM_CACHE_DIR=/tmp/miopen-cache
  #     - MIOPEN_FIND_MODE=1
  #     # Hugging Face cache configuration
  #     - HF_HOME=/data/hf_cache
  #     - TRANSFORMERS_CACHE=/data/hf_cache
  #     - HUGGINGFACE_HUB_CACHE=/data/hf_cache/hub
  #     # Qwen3-TTS configuration
  #     - QWEN_MODEL=${QWEN_MODEL:-Qwen/Qwen3-TTS-12Hz-0.6B-CustomVoice}
  #     - QWEN_VOICE_INSTRUCT=${QWEN_VOICE_INSTRUCT:-}
  #     - QWEN_SPEAKER=${QWEN_SPEAKER:-Ryan}
  #     - QWEN_LANGUAGE=${QWEN_LANGUAGE:-Auto}
  #     - QWEN_DEVICE=${QWEN_DEVICE:-cuda:0}
  #     - QWEN_DTYPE=${QWEN_DTYPE:-bfloat16}
  #     - QWEN_FLASH_ATTENTION=${QWEN_FLASH_ATTENTION:-true}
  #     - QWEN_SAMPLES_PER_CHUNK=${QWEN_SAMPLES_PER_CHUNK:-1024}
  #     - QWEN_CACHE_DIR=/data/models
  #     - QWEN_DEBUG=${QWEN_DEBUG:-true}

  #   # Port for Wyoming protocol
  #   ports:
  #     - "10200:10200"

  #   # Volume for model persistence
  #   volumes:
  #     - ./qwen-data:/data

  #   # Health check
  #   healthcheck:
  #     test: ["CMD", "python3", "-c", "import socket; s=socket.socket(); s.connect(('localhost',10200)); s.close()"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 3
  #     start_period: 120s

  # wyoming-chatterbox-turbo: (Trying Kokoro first)
  # wyoming-chatterbox-turbo:
  #   build:
  #     context: ./chatterbox-turbo
  #     dockerfile: Dockerfile
  #     args:
  #       HSA_OVERRIDE_GFX_VERSION: ${HSA_OVERRIDE_GFX_VERSION:-11.5.1}
  #   container_name: wyoming-chatterbox-turbo-rocm
  #   restart: unless-stopped

  #   # GPU device access - required for ROCm
  #   devices:
  #     - /dev/kfd:/dev/kfd
  #     - /dev/dri:/dev/dri

  #   # Security options for GPU access
  #   security_opt:
  #     - seccomp:unconfined

  #   # Group access for GPU
  #   group_add:
  #     - video

  #   # Environment variables
  #   environment:
  #     - HSA_OVERRIDE_GFX_VERSION=${HSA_OVERRIDE_GFX_VERSION:-11.5.1}
  #     - HIP_VISIBLE_DEVICES=0
  #     - ROCM_VERSION=7.1.1
  #     - ROCM_PATH=/opt/rocm
  #     - LD_LIBRARY_PATH=/usr/local/lib:/opt/rocm/lib:/opt/rocm/lib/llvm/lib
  #     # Enable experimental ROCm SDPA optimizations for better performance
  #     - TORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL=1
  #     # MIOpen performance tuning - fix workspace allocation warnings
  #     - MIOPEN_USER_DB_PATH=/tmp/miopen-cache
  #     - MIOPEN_CUSTOM_CACHE_DIR=/tmp/miopen-cache
  #     - MIOPEN_FIND_MODE=1
  #     # Hugging Face cache configuration
  #     - HF_HOME=/data/hf_cache
  #     - TRANSFORMERS_CACHE=/data/hf_cache
  #     - HUGGINGFACE_HUB_CACHE=/data/hf_cache/hub
  #     - HF_TOKEN=${HF_TOKEN}
  #     # Chatterbox Turbo configuration
  #     - CHATTERBOX_DEVICE=${CHATTERBOX_DEVICE:-cuda:0}
  #     - CHATTERBOX_SAMPLES_PER_CHUNK=${CHATTERBOX_SAMPLES_PER_CHUNK:-1024}
  #     - CHATTERBOX_CACHE_DIR=/data/models
  #     - CHATTERBOX_DEBUG=${CHATTERBOX_DEBUG:-true}

  #   # Port for Wyoming protocol (using 10201 to avoid conflict with qwen)
  #   ports:
  #     - "10201:10200"

  #   # Volume for model persistence
  #   volumes:
  #     - ./chatterbox-data:/data

  #   # Health check
  #   healthcheck:
  #     test: ["CMD", "python3", "-c", "import socket; s=socket.socket(); s.connect(('localhost',10200)); s.close()"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 3
  #     start_period: 90s

  # wyoming-pocket-tts: (Trying Kokoro first)
  # wyoming-pocket-tts:
  #   build:
  #     context: ./pocket-tts
  #     dockerfile: Dockerfile
  #     args:
  #       HSA_OVERRIDE_GFX_VERSION: ${HSA_OVERRIDE_GFX_VERSION:-11.5.1}
  #   container_name: wyoming-pocket-tts-rocm
  #   restart: unless-stopped

  #   # GPU device access - required for ROCm base image
  #   # Note: Pocket TTS is CPU-only and doesn't benefit from GPU
  #   devices:
  #     - /dev/kfd:/dev/kfd
  #     - /dev/dri:/dev/dri

  #   # Security options for GPU access
  #   security_opt:
  #     - seccomp:unconfined

  #   # Group access for GPU
  #   group_add:
  #     - video
  #     - render

  #   # Environment variables
  #   environment:
  #     - HSA_OVERRIDE_GFX_VERSION=${HSA_OVERRIDE_GFX_VERSION:-11.5.1}
  #     - ROCM_VERSION=7.1.1
  #     - ROCM_PATH=/opt/rocm
  #     - LD_LIBRARY_PATH=/usr/local/lib:/opt/rocm/lib:/opt/rocm/lib/llvm/lib
  #     # Hugging Face cache configuration
  #     - HF_HOME=/data/hf_cache
  #     - TRANSFORMERS_CACHE=/data/hf_cache
  #     - HUGGINGFACE_HUB_CACHE=/data/hf_cache/hub
  #     # Pocket TTS configuration
  #     # Built-in voices: alba, marius, javert, jean, fantine, cosette, eponine, azelma
  #     # For voice cloning, use HF URLs or .wav file paths (requires gated model access)
  #     - POCKET_VOICE=${POCKET_VOICE:-fantine}
  #     - POCKET_CACHE_DIR=/data/models
  #     - POCKET_DEBUG=${POCKET_DEBUG:-true}

  #   # Port for Wyoming protocol (using 10202 to avoid conflicts)
  #   ports:
  #     - "10202:10202"

  #   # Volume for model persistence
  #   volumes:
  #     - ./pocket-data:/data

  #   # Health check
  #   healthcheck:
  #     test: ["CMD", "python3", "-c", "import socket; s=socket.socket(); s.connect(('localhost',10202)); s.close()"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 3
  #     start_period: 60s

  # wyoming-kokoro-tts: (Using local kokoro service instead)
  # wyoming-kokoro-tts:
  #   build:
  #     context: ./kokoro
  #     dockerfile: Dockerfile
  #   container_name: wyoming-kokoro-tts-rocm
  #   restart: unless-stopped

  #   # No GPU devices - proxies to external API

  #   # Environment variables
  #   environment:
  #     - KOKORO_API_URL=${KOKORO_API_URL:-http://10.0.3.23:8880/v1}
  #     - KOKORO_VOICE=${KOKORO_VOICE:-af_bella}
  #     - KOKORO_SPEED=${KOKORO_SPEED:-1.0}
  #     - KOKORO_TIMEOUT=${KOKORO_TIMEOUT:-30}
  #     - KOKORO_DEBUG=${KOKORO_DEBUG:-false}

  #   # Port for Wyoming protocol (using 10203 to avoid conflicts)
  #   ports:
  #     - "10203:10203"

  #   # Volume for potential caching needs
  #   volumes:
  #     - ./kokoro-data:/data

  #   # Health check
  #   healthcheck:
  #     test: ["CMD", "python3", "-c", "import socket; s=socket.socket(); s.connect(('localhost',10203)); s.close()"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 3
  #     start_period: 10s

#  wyoming-moonshine:
#    build:
#      context: ./moonshine
#      dockerfile: Dockerfile
#    container_name: wyoming-moonshine
#    restart: unless-stopped

    # Environment variables
#    environment:
      # Hugging Face cache configuration
#      - HF_HOME=/data/hf_cache
#      - HUGGINGFACE_HUB_CACHE=/data/hf_cache/hub
      # Moonshine configuration
      # Models: moonshine/tiny (27M params) or moonshine/base (62M params)
#      - MOONSHINE_MODEL=${MOONSHINE_MODEL:-moonshine/tiny}
#      - MOONSHINE_DEBUG=${MOONSHINE_DEBUG:-false}

    # Port for Wyoming protocol (10302 for STT)
#    ports:
#      - "10302:10302"

    # Volume for model persistence
#    volumes:
#      - ./moonshine-data:/data

    # Health check
#    healthcheck:
#      test: ["CMD", "python3", "-c", "import socket; s=socket.socket(); s.connect(('localhost',10302)); s.close()"]
#      interval: 30s
#      timeout: 10s
#      retries: 3
#      start_period: 60s

  # wyoming-moonshine: (Trying Whisper first)
  # wyoming-moonshine:
  #   build:
  #     context: ./moonshine
  #     dockerfile: Dockerfile
  #   container_name: wyoming-moonshine
  #   restart: unless-stopped

  #   # Environment variables
  #   environment:
  #     - HF_HOME=/data/hf_cache
  #     - HUGGINGFACE_HUB_CACHE=/data/hf_cache/hub
  #     - MOONSHINE_MODEL=${MOONSHINE_MODEL:-moonshine/tiny}
  #     - MOONSHINE_DEBUG=${MOONSHINE_DEBUG:-false}

  #   # Port for Wyoming protocol (10302 for STT)
  #   ports:
  #     - "10302:10302"

  #   # Health check
  #   healthcheck:
  #     test: ["CMD", "python3", "-c", "import socket; s=socket.socket(); s.connect(('localhost',10302)); s.close()"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 3
  #     start_period: 60s

  # wyoming-voxtral: (Not supporting - forked, has ROCm/vLLM issues)
  # wyoming-voxtral:
  #   build:
  #     context: ./voxtral
  #     dockerfile: Dockerfile
  #     args:
  #       HSA_OVERRIDE_GFX_VERSION: ${HSA_OVERRIDE_GFX_VERSION:-11.5.1}
  #   container_name: wyoming-voxtral-rocm
  #   restart: unless-stopped

  #   # GPU device access - required for vLLM with ROCm
  #   devices:
  #     - /dev/kfd:/dev/kfd
  #     - /dev/dri:/dev/dri

  #   # Security options for GPU access
  #   security_opt:
  #     - seccomp:unconfined

  #   # Group access for GPU
  #   group_add:
  #     - video
  #     - render

  #   # Environment variables
  #   environment:
  #     - HSA_OVERRIDE_GFX_VERSION=${HSA_OVERRIDE_GFX_VERSION:-11.5.1}
  #     - HIP_VISIBLE_DEVICES=0
  #     - ROCM_VERSION=7.1.1
  #     - ROCM_PATH=/opt/rocm
  #     - LD_LIBRARY_PATH=/usr/local/lib:/opt/rocm/lib:/opt/rocm/lib/llvm/lib
  #     - CUDA_VISIBLE_DEVICES=0
  #     - ROCR_VISIBLE_DEVICES=0
  #     - GPU_DEVICE_ORDINAL=0
  #     - VLLM_DISABLE_COMPILE_CACHE=1
  #     - VLLM_USE_RAY_COMPILED_DAG=0
  #     - VLLM_WORKER_MULTIPROC_METHOD=spawn
  #     - VLLM_USE_TRITON_FLASH_ATTN=0
  #     - VLLM_ATTENTION_BACKEND=XFORMERS
  #     - RAY_EXPERIMENTAL_NOSET_ROCR_VISIBLE_DEVICES=1
  #     - RAY_EXPERIMENTAL_NOSET_HIP_VISIBLE_DEVICES=1
  #     - HF_HOME=/data/hf_cache
  #     - TRANSFORMERS_CACHE=/data/hf_cache
  #     - HUGGINGFACE_HUB_CACHE=/data/hf_cache/hub
  #     - VOXTRAL_MODEL=${VOXTRAL_MODEL:-mistralai/Voxtral-Mini-4B-Realtime-2602}
  #     - VOXTRAL_LANGUAGE=${VOXTRAL_LANGUAGE:-en}
  #     - VOXTRAL_GPU_MEMORY=${VOXTRAL_GPU_MEMORY:-0.9}
  #     - VOXTRAL_DEBUG=${VOXTRAL_DEBUG:-false}

  #   # Port for Wyoming protocol (using 10301 for STT)
  #   ports:
  #     - "10301:10301"

  #   # Health check
  #   healthcheck:
  #     test: ["CMD", "python3", "-c", "import socket; s=socket.socket(); s.connect(('localhost',10301)); s.close()"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 3
  #     start_period: 180s

  # =============================================================================
  # Voice Pipeline: STT -> LLM -> TTS
  # =============================================================================

  llama-cpp:
    build:
      context: ./llama-cpp
      dockerfile: Dockerfile
    container_name: llama-cpp
    restart: unless-stopped

    # GPU device access - required for ROCm
    devices:
      - /dev/kfd:/dev/kfd
      - /dev/dri:/dev/dri

    # Security options for GPU access
    security_opt:
      - seccomp:unconfined

    # Group access for GPU
    group_add:
      - video
      - render

    # Environment variables
    environment:
      - HSA_OVERRIDE_GFX_VERSION=${HSA_OVERRIDE_GFX_VERSION:-11.5.1}
      - HIP_VISIBLE_DEVICES=0
      - MODEL_NAME=${MODEL_NAME:-qwen3-next}

    # Port for llama.cpp HTTP API
    ports:
      - "8080:8080"

    # Volume for model cache
    volumes:
      - ~/.cache/huggingface/hub:/root/.cache/huggingface/hub

  kokoro:
    build:
      context: ./kokoro-fastapi-strix
      dockerfile: Dockerfile
    container_name: kokoro
    restart: unless-stopped

    # GPU device access - required for ROCm
    devices:
      - /dev/kfd:/dev/kfd
      - /dev/dri:/dev/dri

    # video + render groups give the container access to the GPU nodes
    group_add:
      - video
      - render

    # Security options for GPU access
    security_opt:
      - seccomp:unconfined

    environment:
      # gfx1151 is natively supported in rocm/pytorch 7.1.1
      # HSA_OVERRIDE_GFX_VERSION: "11.0.0"
      # Critical for Strix Halo APU unified memory
      - HSA_ENABLE_SDMA=0
      # Use Triton as flash-attention backend
      - FLASH_ATTENTION_TRITON_AMD_ENABLE=1
      # Disable TunableOp
      - PYTORCH_TUNABLEOP_ENABLED=0
      # Kokoro device selection
      - KOKORO_DEVICE=cuda

    # Port for Kokoro HTTP API
    ports:
      - "8880:8880"

    # Models will be downloaded to HuggingFace cache on first run
    volumes:
      - hf-cache:/root/.cache/huggingface

  orchestrator:
    build:
      context: ./orchestrator
      dockerfile: Dockerfile
    container_name: orchestrator
    restart: unless-stopped

    # Environment variables
    environment:
      - HTTP_PORT=10501
      - LLAMA_API_URL=http://llama-cpp:8080
      - KOKORO_API_URL=http://kokoro:8880
      - MODEL_NAME=${MODEL_NAME:-qwen3-next}
      - KOKORO_VOICE=${KOKORO_VOICE:-onyx}
      - MAX_CONTEXT_PCT=0.90
      - ROLL_TO_PCT=0.80
      - MESSAGE_TTL_MS=600000
      - WHISPER_URL=http://whisper:10300

    # Port for REST API
    ports:
      - "10501:10501"

    depends_on:
      - llama-cpp
      - kokoro
      - whisper

  # =============================================================================
  # End Voice Pipeline
  # =============================================================================

  # =============================================================================
  # Frontend (Vue + Vite)
  # =============================================================================

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    container_name: voice-frontend
    restart: unless-stopped

    ports:
      - "3000:3000"

    environment:
      - NODE_ENV=development

  # =============================================================================
  # End Frontend
  # =============================================================================

volumes:
  hf-cache:
