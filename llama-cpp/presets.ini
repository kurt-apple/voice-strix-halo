[model.gemma]
model = /home/rocky/.cache/llama.cpp/blah
ctx_size = 8192
n_gpu_layers = 999
embedding = true
# Capabilities: embed

[model.qwen3-coder]
model = /home/rocky/.cache/llama.cpp/blah
ctx_size = 256000
n_gpu_layers = 999
# Capabilities: chat, code, tools

[model.qwen3-coder-next]
model = /home/rocky/.cache/llama.cpp/blah
ctx_size = 256000
n_gpu_layers = 999
# Capabilities: chat, code, tools

[model.qwen3-next]
model = /home/rocky/.cache/llama.cpp/blah
ctx_size = 256000
n_gpu_layers = 999
# Capabilities: chat, code, tools, reasoning

